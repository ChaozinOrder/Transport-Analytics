{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bec173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd \n",
    "import json \n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import PyPDF2\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9bacd",
   "metadata": {},
   "source": [
    "SAMPLE CONVERSION OF PDF TO CSV TABULAR DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_table_to_csv(pdf_path, csv_path):\n",
    "    \"\"\"\n",
    "    Extracts tables from a PDF using pdfplumber, converts them to a pandas DataFrame,\n",
    "    and saves the DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        csv_path (str): The path where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if data was successfully extracted and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    all_extracted_data = []\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables()\n",
    "\n",
    "                if tables:\n",
    "                    for table in tables:\n",
    "                         if table and len(table) > 1:\n",
    "                            header = [h.replace('\\n', ' ').strip() if h else '' for h in table[0]]\n",
    "                            for row in table[1:]:\n",
    "                                row_data = {}\n",
    "                                padded_row = row + [None] * (len(header) - len(row))\n",
    "                                for i, col_name in enumerate(header):\n",
    "                                    cell_value = padded_row[i]\n",
    "                                    row_data[col_name] = cell_value.replace('\\n', ' ').strip() if isinstance(cell_value, str) else cell_value\n",
    "                                all_extracted_data.append(row_data)\n",
    "                else:\n",
    "                    print(f\"No tables automatically detected on page {page.page_number}. You might need custom text parsing.\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {pdf_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- New part for CSV ---\n",
    "    if all_extracted_data:\n",
    "        df = pd.DataFrame(all_extracted_data)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Successfully extracted data and saved to {csv_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No data extracted from tables.\")\n",
    "        return False\n",
    "\n",
    "pdf_file_path = 'IRI-07314-Kollam - SSS Hubballi Special Fare Sabarimala Special.pdf'\n",
    "csv_output_path = 'output_schedule.csv' \n",
    "\n",
    "success = pdf_table_to_csv(pdf_file_path, csv_output_path)\n",
    "\n",
    "if not success:\n",
    "    print(\"Process failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1eeed",
   "metadata": {},
   "source": [
    "TAKING TRAIN DATA FROM PDFs AND TABULATING IT AS DIFFERENT PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_number_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts a cleaned train number (numeric only, no leading zeros) from a PDF filename.\n",
    "    Finds the first sequence of digits in the filename and removes leading zeros.\n",
    "    Discards any surrounding alphabets.\n",
    "    E.g., 'xx1462.pdf' -> '1462', '01462x.pdf' -> '1462', 'IRI-07043-Name.pdf' -> '7043', 'train123.pdf' -> '123', '1462XXX.pdf' -> '1462'\n",
    "    Returns None if no digit sequence is found in the filename.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    if match:\n",
    "        numeric_str = match.group(1)\n",
    "        try:\n",
    "            cleaned_train_num = str(int(numeric_str))\n",
    "            return cleaned_train_num\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not convert '{numeric_str}' to integer from filename {filename}\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def pdf_table_to_csv(pdf_path, csv_path):\n",
    "    \"\"\"\n",
    "    Extracts tables from a PDF using pdfplumber, converts them to a pandas DataFrame,\n",
    "    renames the first column to 'Zone', and saves the DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        csv_path (str): The path where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if data was successfully extracted and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    all_extracted_data = []\n",
    "    pdf_filename = os.path.basename(pdf_path) \n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                tables = page.extract_tables(table_settings={})\n",
    "\n",
    "                if tables:\n",
    "                    for table in tables:\n",
    "                         if table and len(table) > 1:\n",
    "                            header = [h.replace('\\n', ' ').strip() if h else '' for h in table[0]]\n",
    "                            for row in table[1:]:\n",
    "                                row_data = {}\n",
    "                                padded_row = row + [None] * (len(header) - len(row))\n",
    "                                for i, col_name in enumerate(header):\n",
    "                                    cell_value = padded_row[i]\n",
    "                                    row_data[col_name] = cell_value.replace('\\n', ' ').strip() if isinstance(cell_value, str) else cell_value\n",
    "                                all_extracted_data.append(row_data)\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error in pdf_table_to_csv: PDF file not found at {pdf_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred processing {pdf_filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "    if all_extracted_data:\n",
    "        try:\n",
    "            df = pd.DataFrame(all_extracted_data)\n",
    "            if not df.empty and not df.columns.empty:\n",
    "                current_columns = list(df.columns)\n",
    "                current_columns[0] = 'Zone'\n",
    "                df.columns = current_columns\n",
    "\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV {os.path.basename(csv_path)}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "pdf_folder = \"pdfs\" \n",
    "output_csv_folder = \"extracted_tables_csv\" \n",
    "\n",
    "if not os.path.exists(output_csv_folder):\n",
    "    os.makedirs(output_csv_folder)\n",
    "    print(f\"Created output folder: {output_csv_folder}\")\n",
    "\n",
    "all_files_in_folder = os.listdir(pdf_folder)\n",
    "\n",
    "pdf_files = [f for f in all_files_in_folder if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "if pdf_files:\n",
    "    print(f\"Found {len(pdf_files)} PDF files in '{pdf_folder}'. Starting table extraction to CSV...\")\n",
    "    pdf_files.sort()\n",
    "\n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for i, filename in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        print(f\"Processing file {i+1}/{len(pdf_files)}: {filename}...\")\n",
    "        train_number = extract_train_number_from_filename(filename)\n",
    "\n",
    "        if train_number:\n",
    "            csv_filename = f\"{train_number}_table_data.csv\"\n",
    "            csv_output_path = os.path.join(output_csv_folder, csv_filename)\n",
    "            success = pdf_table_to_csv(pdf_path, csv_output_path)\n",
    "\n",
    "            if success:\n",
    "                print(f\"  Successfully saved table data to {csv_filename}\")\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"  Table data extraction failed or no data found in {filename}.\")\n",
    "                failed_count += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"  Could not extract a valid train number from filename: {filename}. Skipping table extraction for this file.\")\n",
    "            failed_count += 1 \n",
    "\n",
    "    print(\"\\n--- Automation Summary ---\")\n",
    "    print(f\"Total PDF files found: {len(pdf_files)}\")\n",
    "    print(f\"Files processed successfully (CSV saved): {processed_count}\")\n",
    "    print(f\"Files failed or skipped: {failed_count}\")\n",
    "    print(f\"Output CSV files saved to: {output_csv_folder}\")\n",
    "\n",
    "else:\n",
    "    print(f\"No PDF files found in the '{pdf_folder}' folder to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc5b84",
   "metadata": {},
   "source": [
    "SAMPLE DATA PARSING FROM PDF TO CHECK IF ITS WORKING FOR ONE TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d71d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_numbers_from_csv(train_num_csv):\n",
    "    \"\"\"Reads train numbers from a CSV file.\"\"\"\n",
    "    train_numbers = []\n",
    "    try:\n",
    "        with open(train_num_csv, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader, None)  \n",
    "            for row in reader:\n",
    "                train_numbers.append(row[0])  \n",
    "        return train_numbers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {train_num_csv}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "def find_pdf_by_train_number(train_number, pdf_folder):\n",
    "    \"\"\"Finds a PDF file in a folder that contains the train number in its name.\"\"\"\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            #  Adjust this regex if your filenames have a different pattern\n",
    "            if re.search(rf\"{train_number}\", filename, re.IGNORECASE):\n",
    "                return os.path.join(pdf_folder, filename)\n",
    "    return None\n",
    "\n",
    "def extract_train_data(pdf_path, train_num):\n",
    "    \"\"\"Extracts train data from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or ''\n",
    "\n",
    "    data = {}\n",
    "    data['Train Number'] = train_num\n",
    "\n",
    "    match = re.search(r'(\\d+)/([^-]+) - ([^\\n]+)', text)\n",
    "    if match:\n",
    "        data['Full Train Name'] = match.group(2).strip() + \" - \" + match.group(3).strip()\n",
    "        data['Train Type'] = \"Mail/Express\" \n",
    "    else:\n",
    "        data['Full Train Name'] = None\n",
    "        data['Train Type'] = None\n",
    "\n",
    "    match = re.search(r'(\\d+h \\d+m) - (\\d+ km) - (\\d+ halts)', text)\n",
    "    if match:\n",
    "        duration_str = match.group(1)\n",
    "        hours, minutes = map(int, duration_str.replace('h ', ' ').replace('m', '').split())\n",
    "        data['Duration (minutes)'] = hours * 60 + minutes\n",
    "        data['Distance (km)'] = int(match.group(2).replace(' km', ''))\n",
    "        data['Number of Halts'] = int(match.group(3).replace(' halts', ''))\n",
    "    else:\n",
    "        data['Duration (minutes)'] = None\n",
    "        data['Distance (km)'] = None\n",
    "        data['Number of Halts'] = None\n",
    "\n",
    "    match = re.search(r'Departs ([\\w, ]+)', text)\n",
    "    if match:\n",
    "        data['Days of Operation'] = match.group(1)\n",
    "    else:\n",
    "        data['Days of Operation'] = None\n",
    "\n",
    "    stations_data = []\n",
    "    start_extract = False\n",
    "    for line in text.split('\\n'):\n",
    "        if '\"#\",\"Code\",\"Station Name\",\"Arr\",\"Dep\",\"Halt\",\"PF\",\"Day\",\"Km\",\"Spd\",\"Elv\",\"Zone\"' in line:\n",
    "            start_extract = True\n",
    "            continue\n",
    "        if start_extract:\n",
    "            values = line.split('\",\"')\n",
    "            if len(values) >= 12:  \n",
    "                try:\n",
    "                    station = {\n",
    "                        '#': values[0].replace('\"', ''),\n",
    "                        'Code': values[1].replace('\"', ''),\n",
    "                        'Station Name': values[2].replace('\"', ''),\n",
    "                        'Arr': values[3].replace('\"', ''),\n",
    "                        'Dep': values[4].replace('\"', ''),\n",
    "                        'Halt': values[5].replace('\"', ''),\n",
    "                        'Km': values[8].replace('\"', ''),\n",
    "                        'Spd': values[9].replace('\"', ''),\n",
    "                        'Elv': values[10].replace('\"', '')\n",
    "                    }\n",
    "                    stations_data.append(station)\n",
    "                except IndexError:\n",
    "                    print(f\"Incomplete data in line: {line}\")\n",
    "    return data\n",
    "\n",
    "train_num_csv = \"Unique_Trains_in_Passengers - Sheet1.csv\"\n",
    "pdf_folder = \"pdfs\"\n",
    "\n",
    "train_numbers = read_train_numbers_from_csv(train_num_csv)\n",
    "\n",
    "if train_numbers:\n",
    "    first_train_number = train_numbers[0]  \n",
    "    pdf_path = find_pdf_by_train_number(first_train_number, pdf_folder)\n",
    "\n",
    "    if pdf_path:\n",
    "        extracted_data = extract_train_data(pdf_path, first_train_number)\n",
    "        print(\"Extracted Data:\\n\", extracted_data)  \n",
    "    else:\n",
    "        print(f\"No PDF found for train number: {first_train_number}\")\n",
    "else:\n",
    "    print(\"No train numbers found in the CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2086573",
   "metadata": {},
   "source": [
    "AUTOMATION FOR ALL TRAINS TO GET EXTRACTED DATA FROM PDFS - INCLUDING THE FOLLOWING \n",
    "Extracted Data:\n",
    " {'Train Number': '1462', 'Full Train Name': 'Kanniyakumari - Mumbai CSMT Special Fare Summer Special', 'Train Type': 'Mail/Express', 'Duration (minutes)': 2205, 'Distance (km)': 1826, 'Number of Halts': 31, 'Days of Operation': 'Thu '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90875fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_number_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts a cleaned train number (numeric only, no leading zeros) from a PDF filename.\n",
    "    Finds the first sequence of digits in the filename and removes leading zeros.\n",
    "    Discards any surrounding alphabets.\n",
    "    E.g., 'xx1462.pdf' -> '1462', '01462x.pdf' -> '1462', 'IRI-07043-Name.pdf' -> '7043', 'train123.pdf' -> '123', '1462XXX.pdf' -> '1462'\n",
    "    Returns None if no digit sequence is found in the filename.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    if match:\n",
    "        numeric_str = match.group(1)\n",
    "        cleaned_train_num = str(int(numeric_str))\n",
    "        return cleaned_train_num\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_train_data_from_pdf_content(pdf_path, train_num):\n",
    "    \"\"\"\n",
    "    Extracts high-level train data from the introductory text of a PDF file.\n",
    "    Attempts to extract Name, Type, Duration, Distance, Halts, Days.\n",
    "    Initializes placeholders for table-derived data.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The full path to the PDF file.\n",
    "        train_num (str): The cleaned train number extracted from the filename.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted and initialized train information.\n",
    "    \"\"\"\n",
    "    data = {'Train Number': train_num,\n",
    "            'Full Train Name': None,\n",
    "            'Train Type': None, \n",
    "            'Duration (minutes)': None,\n",
    "            'Distance (km)': None,\n",
    "            'Number of Halts': None,\n",
    "            'Days of Operation': None}\n",
    "\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            if len(reader.pages) > 0:\n",
    "                 text = reader.pages[0].extract_text() or ''\n",
    "\n",
    "        match_name = re.search(rf'{re.escape(str(train_num))}/(.+?)(?: Type: |\\n)', text)\n",
    "        if match_name:\n",
    "             data['Full Train Name'] = match_name.group(1).strip()\n",
    "        else:\n",
    "            first_line_match = re.match(r'.+/(.+)', text.split('\\n')[0])\n",
    "            data['Full Train Name'] = first_line_match.group(1).strip() if first_line_match else None\n",
    "\n",
    "        match_type = re.search(r'Type:\\s*(.+?)(?: Zone:|\\n|$)', text)\n",
    "        if match_type:\n",
    "            data['Train Type'] = match_type.group(1).strip()\n",
    "        else:\n",
    "            data['Train Type'] = None\n",
    "\n",
    "        match_details = re.search(r'(\\d+h\\s*\\d+m)\\s*-?\\s*(\\d+)\\s*km\\s*-?\\s*(\\d+)\\s*halts', text)\n",
    "        if match_details:\n",
    "            duration_str = match_details.group(1)\n",
    "            try:\n",
    "                duration_str_cleaned = duration_str.replace('h', ' ').replace('m', '').strip()\n",
    "                hours, minutes = map(int, duration_str_cleaned.split())\n",
    "                data['Duration (minutes)'] = hours * 60 + minutes\n",
    "            except ValueError:\n",
    "                 data['Duration (minutes)'] = None\n",
    "\n",
    "            try:\n",
    "                data['Distance (km)'] = int(match_details.group(2))\n",
    "            except ValueError:\n",
    "                 data['Distance (km)'] = None\n",
    "\n",
    "            try:\n",
    "                data['Number of Halts'] = int(match_details.group(3))\n",
    "            except ValueError:\n",
    "                 data['Number of Halts'] = None\n",
    "        else:\n",
    "             data['Duration (minutes)'] = None\n",
    "             data['Distance (km)'] = None\n",
    "             data['Number of Halts'] = None\n",
    "\n",
    "\n",
    "        match_days = re.search(r'Departs ([\\w, ]+)', text)\n",
    "        if match_days:\n",
    "            data['Days of Operation'] = match_days.group(1).strip()\n",
    "        else:\n",
    "             match_days_alt = re.search(r'Zone: \\w+ Departs ([\\w, ]+)', text)\n",
    "             if match_days_alt:\n",
    "                  data['Days of Operation'] = match_days_alt.group(1).strip()\n",
    "             else:\n",
    "                data['Days of Operation'] = None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "        pass \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "        pass \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "pdf_folder = \"pdfs\" \n",
    "output_csv_file = \"all_pdfs_extracted_data.csv\" \n",
    "\n",
    "all_trains_data = [] \n",
    "\n",
    "all_files_in_folder = os.listdir(pdf_folder)\n",
    "\n",
    "pdf_files = [f for f in all_files_in_folder if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "if pdf_files:\n",
    "    print(f\"Found {len(pdf_files)} PDF files in '{pdf_folder}'.\")\n",
    "    pdf_files.sort()\n",
    "\n",
    "    for i, filename in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        print(f\"Processing file {i+1}/{len(pdf_files)}: {filename}...\")\n",
    "\n",
    "        train_number = extract_train_number_from_filename(filename)\n",
    "\n",
    "        if train_number:\n",
    "            print(f\"  Extracted train number from filename: {train_number}\")\n",
    "            extracted_data = extract_train_data_from_pdf_content(pdf_path, train_number)\n",
    "            all_trains_data.append(extracted_data)\n",
    "        else:\n",
    "            print(f\"  Could not extract a valid train number from filename: {filename}. Skipping this file.\")\n",
    "            all_trains_data.append({'Train Number': None, \n",
    "                                   'Full Train Name': None, 'Train Type': None,\n",
    "                                   'Duration (minutes)': None, 'Distance (km)': None,\n",
    "                                   'Number of Halts': None, 'Days of Operation': None,\n",
    "                                   'Stations': [], 'Origin': None, 'Destination': None,\n",
    "                                   'Dep Time': None, 'Arr Time': None,\n",
    "                                   'Total Halt Time (minutes)': None, 'Average Speed (km/h)': None,\n",
    "                                   'Average Elevation (m)': None, 'Zone of Origin': None,\n",
    "                                   'Zone of Destination': None})\n",
    "\n",
    "    if all_trains_data:\n",
    "        print(f\"\\nCollected data for {len(all_trains_data)} files. Saving to CSV...\")\n",
    "        df_output = pd.DataFrame(all_trains_data)\n",
    "\n",
    "        output_columns = [\n",
    "            'Train Number', 'Full Train Name', 'Train Type', 'Duration (minutes)',\n",
    "            'Distance (km)', 'Number of Halts', 'Days of Operation', 'Stations',\n",
    "            'Origin', 'Destination', 'Dep Time', 'Arr Time', 'Total Halt Time (minutes)',\n",
    "            'Average Speed (km/h)', 'Average Elevation (m)', 'Zone of Origin', 'Zone of Destination'\n",
    "        ]\n",
    "\n",
    "        df_output = df_output.reindex(columns=output_columns)\n",
    "\n",
    "        try:\n",
    "            df_output.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully saved extracted data to {output_csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to CSV {output_csv_file}: {e}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data was extracted from any PDF file to save.\")\n",
    "\n",
    "else:\n",
    "    print(f\"No PDF files found in the '{pdf_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74de2a",
   "metadata": {},
   "source": [
    "POPULATING DERIVED DATA FROM THE TRAIN SPECIFIC CSVS TO MAIN CSV OF TRAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_csv_path = \"all_pdfs_extracted_data.csv\"\n",
    "extracted_tables_folder = \"extracted_tables_csv\"\n",
    "\n",
    "print(f\"Attempting to populate data into the main CSV: {main_csv_path}\")\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(main_csv_path):\n",
    "        print(f\"Error: Main CSV file not found at {main_csv_path}\")\n",
    "    else:\n",
    "        df_main = pd.read_csv(main_csv_path)\n",
    "\n",
    "        if 'Stations' in df_main.columns:\n",
    "            df_main = df_main.drop(columns=['Stations'])\n",
    "            print(\"Dropped the 'Stations' column from the main DataFrame.\")\n",
    "        else:\n",
    "            print(\"Warning: 'Stations' column not found in the main DataFrame.\")\n",
    "\n",
    "        # --- Check if required columns exist in the main DataFrame ---\n",
    "        required_main_cols = ['Train Number', 'Origin', 'Destination']\n",
    "        if not all(col in df_main.columns for col in required_main_cols):\n",
    "            missing = [col for col in required_main_cols if col not in df_main.columns]\n",
    "            print(f\"Error: Missing required columns in the main CSV: {missing}\")\n",
    "        elif df_main.empty:\n",
    "             print(\"Main CSV DataFrame is empty.\")\n",
    "        else:\n",
    "            print(f\"Processing {len(df_main)} trains to populate Origin and Destination...\")\n",
    "            for index, row in df_main.iterrows():\n",
    "                train_number = row['Train Number']\n",
    "\n",
    "                if pd.notna(train_number) and train_number != '':\n",
    "                    train_csv_filename = f\"{train_number}_table_data.csv\"\n",
    "                    train_csv_path = os.path.join(extracted_tables_folder, train_csv_filename)\n",
    "\n",
    "                    if os.path.exists(train_csv_path):\n",
    "                        try:\n",
    "                            df_train = pd.read_csv(train_csv_path)\n",
    "\n",
    "                            if not df_train.empty and 'Station Name' in df_train.columns:\n",
    "                                origin_station = df_train.iloc[0]['Station Name']\n",
    "\n",
    "                                destination_station = df_train.iloc[-2]['Station Name']\n",
    "                                df_main.loc[index, 'Origin'] = origin_station\n",
    "                                df_main.loc[index, 'Destination'] = destination_station\n",
    "                                print(f\"  Updated Origin and Destination for Train {train_number}\") \n",
    "\n",
    "                            else:\n",
    "                                print(f\"  Warning: Train-specific CSV {train_csv_filename} is empty or missing 'Station Name' column.\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Error reading or processing train-specific CSV {train_csv_filename}: {e}\")\n",
    "                    else:\n",
    "                        pass \n",
    "\n",
    "                else:\n",
    "                    print(f\"  Warning: Skipping row {index} in main CSV due to invalid Train Number: {train_number}\")\n",
    "            print(\"\\nFinished processing. Saving updated main CSV...\")\n",
    "            df_main.to_csv(main_csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully updated and saved the main CSV file: {main_csv_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"An error occurred because the main CSV file was not found at {main_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the population process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_csv_path = \"all_pdfs_extracted_data.csv\"\n",
    "extracted_tables_folder = \"extracted_tables_csv\"\n",
    "\n",
    "print(f\"Starting population of Dep Time, Arr Time, and Total Halt Time in main CSV: {main_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    if not df_main.empty:\n",
    "        print(f\"Processing {len(df_main)} trains...\")\n",
    "        for index, row in df_main.iterrows():\n",
    "            train_number = row['Train Number']\n",
    "            if pd.notna(train_number) and str(train_number).strip() != '':\n",
    "                train_csv_filename = f\"{str(train_number).strip()}_table_data.csv\"\n",
    "                train_csv_path = os.path.join(extracted_tables_folder, train_csv_filename)\n",
    "                try:\n",
    "                    df_train = pd.read_csv(train_csv_path)\n",
    "                    dep_time_val = df_train.iloc[0]['Dep']\n",
    "                    df_main.loc[index, 'Dep Time'] = str(dep_time_val).strip() if pd.notna(dep_time_val) else None\n",
    "                    arr_time_val = df_train.iloc[-2]['Arr']\n",
    "                    df_main.loc[index, 'Arr Time'] = str(arr_time_val).strip() if pd.notna(arr_time_val) else None\n",
    "                    total_halt_minutes = 0\n",
    "                    try:\n",
    "                        for halt_entry in df_train['Halt'].astype(str):\n",
    "                            if halt_entry.lower() != 'nan' and halt_entry.strip() != '':\n",
    "                                match = re.match(r'(\\d+)m', halt_entry.strip(), re.IGNORECASE)\n",
    "                                if match:\n",
    "                                    total_halt_minutes += int(match.group(1))\n",
    "                    except Exception as e:\n",
    "                         print(f\"  Error calculating total halt time for train {train_number}: {e}\")\n",
    "                         total_halt_minutes = None \n",
    "\n",
    "\n",
    "                    df_main.loc[index, 'Total Halt Time (minutes)'] = total_halt_minutes\n",
    "                    print(f\"  Successfully processed times and halt time for train {train_number}\") # Optional print\n",
    "\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"  Warning: Train-specific CSV not found for train {train_number} at {train_csv_path}. Skipping.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing train-specific CSV {train_csv_filename} for train {train_number}: {e}. Skipping.\")\n",
    "\n",
    "            print(\"\\nFinished processing. Saving updated main CSV...\")\n",
    "            df_main.to_csv(main_csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully updated and saved the main CSV file: {main_csv_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Main CSV DataFrame is empty. Nothing to process.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"An error occurred because the main CSV file was not found at {main_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the population process: {e}\")\n",
    "\n",
    "print(\"Population script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting population of Average Speed and Average Elevation in main CSV: {main_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    required_main_cols = ['Train Number', 'Average Speed (km/h)', 'Average Elevation (m)']\n",
    "    if not all(col in df_main.columns for col in required_main_cols):\n",
    "         missing = [col for col in required_main_cols if col not in df_main.columns]\n",
    "         print(f\"Error: Missing required columns in the main CSV for population: {missing}. Please ensure these columns exist.\")\n",
    "    elif df_main.empty:\n",
    "         print(\"Main CSV DataFrame is empty. Nothing to process.\")\n",
    "    else:\n",
    "        print(f\"Processing {len(df_main)} trains to populate average speed and elevation...\")\n",
    "        for index, row in df_main.iterrows():\n",
    "            train_number = row['Train Number']\n",
    "            if pd.notna(train_number) and str(train_number).strip() != '':\n",
    "                train_csv_filename = f\"{str(train_number).strip()}_table_data.csv\"\n",
    "                train_csv_path = os.path.join(extracted_tables_folder, train_csv_filename)\n",
    "                try:\n",
    "                    df_train = pd.read_csv(train_csv_path)\n",
    "                    df_train['Spd_numeric'] = pd.to_numeric(df_train['Spd'], errors='coerce')\n",
    "                    average_speed = df_train['Spd_numeric'].mean()\n",
    "                    df_main.loc[index, 'Average Speed (km/h)'] = round(average_speed, 2) if pd.notna(average_speed) else None\n",
    "                    df_train['Elv_numeric'] = pd.to_numeric(df_train['Elv'], errors='coerce')\n",
    "                    average_elevation = df_train['Elv_numeric'].median()\n",
    "                    df_main.loc[index, 'Average Elevation (m)'] = round(average_elevation, 2) if pd.notna(average_elevation) else None\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"  Warning: Train-specific CSV not found for train {train_number} at {train_csv_path}. Skipping.\")\n",
    "                    if 'Average Speed (km/h)' in df_main.columns:\n",
    "                         df_main.loc[index, 'Average Speed (km/h)'] = None\n",
    "                    if 'Average Elevation (m)' in df_main.columns:\n",
    "                         df_main.loc[index, 'Average Elevation (m)'] = None\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing train-specific CSV {train_csv_filename} for train {train_number}: {e}. Skipping data extraction for train {train_number}.\")\n",
    "                    if 'Average Speed (km/h)' in df_main.columns:\n",
    "                         df_main.loc[index, 'Average Speed (km/h)'] = None\n",
    "                    if 'Average Elevation (m)' in df_main.columns:\n",
    "                         df_main.loc[index, 'Average Elevation (m)'] = None\n",
    "            print(\"\\nFinished processing. Saving updated main CSV...\")\n",
    "            df_main.to_csv(main_csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"Successfully updated and saved the main CSV file: {main_csv_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"An error occurred because the main CSV file was not found at {main_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the population process: {e}\")\n",
    "\n",
    "print(\"Population script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting population of Zone of Origin and Zone of Destination in main CSV: {main_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    for index, row in df_main.iterrows():\n",
    "        train_number = str(row['Train Number']).strip() \n",
    "        train_csv_filename = f\"{train_number}_table_data.csv\"\n",
    "        train_csv_path = os.path.join(extracted_tables_folder, train_csv_filename)\n",
    "\n",
    "        try:\n",
    "            df_train = pd.read_csv(train_csv_path)\n",
    "\n",
    "            zone_origin = df_train.iloc[0]['Zone']\n",
    "            df_main.loc[index, 'Zone of Origin'] = str(zone_origin).strip() if pd.notna(zone_origin) else None\n",
    "            zone_destination = df_train.iloc[-2]['Zone']\n",
    "            df_main.loc[index, 'Zone of Destination'] = str(zone_destination).strip() if pd.notna(zone_destination) else None\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing train {train_number} (CSV: {train_csv_filename}): {e}. Skipping data extraction for this train.\")\n",
    "\n",
    "\n",
    "    print(\"\\nFinished processing. Saving updated main CSV...\")\n",
    "    df_main.to_csv(main_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"Successfully updated and saved the main CSV file: {main_csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the population process: {e}\")\n",
    "\n",
    "print(\"Population script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6f014",
   "metadata": {},
   "source": [
    "COMBINE WITH TRAIN CAPACITY AND OCCUPANCY NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0834d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = \"Train Data Merged Full Raw 2022 - 2024 - Main.xlsx\"\n",
    "\n",
    "capacity_sheet_identifier = 0 # First sheet (0-indexed)\n",
    "passengers_sheet_identifier = 2 # Third sheet (0-indexed)\n",
    "\n",
    "# Define the column renaming mappings.\n",
    "capacity_rename_map = {\n",
    "    '1A': 'CAP1A',\n",
    "    '2A': 'CAP2A',\n",
    "    '2S': 'CAP2S',\n",
    "    '3A': 'CAP3A',\n",
    "    '3E': 'CAP3E',\n",
    "    'CC': 'CAPCC',\n",
    "    'EA': 'CAPEA',    \n",
    "    'EC': 'CAPEC',\n",
    "    'EV': 'CAPEV',\n",
    "    'FC': 'CAPFC',\n",
    "    'SL': 'CAPSL',\n",
    "    'Grand Capacity': 'CAPTotal' \n",
    "}\n",
    "\n",
    "passengers_rename_map = {\n",
    "    '1A': 'OCC1A',\n",
    "    '2A': 'OCC2A',\n",
    "    '2S': 'OCC2S',\n",
    "    '3A': 'OCC3A',\n",
    "    '3E': 'OCC3E',\n",
    "    'CC': 'OCCCC',\n",
    "    'EA': 'OCCEA',\n",
    "    'EC': 'OCCEC',\n",
    "    'EV': 'OCCEV',\n",
    "    'FC': 'OCCFC',\n",
    "    'SL': 'OCCSL',\n",
    "    'TOTAL': 'OCCTotal' \n",
    "}\n",
    "\n",
    "print(f\"Attempting to read Excel file and rename columns: {excel_file_path}\")\n",
    "\n",
    "try:\n",
    "    df_capacity = pd.read_excel(excel_file_path, sheet_name=capacity_sheet_identifier)\n",
    "    print(f\"Successfully read sheet: {capacity_sheet_identifier} ('Capacity').\")\n",
    "\n",
    "    df_capacity = df_capacity.rename(columns=capacity_rename_map, errors='ignore')\n",
    "    print(\"Renamed columns in Capacity DataFrame.\")\n",
    "\n",
    "    df_passengers = pd.read_excel(excel_file_path, sheet_name=passengers_sheet_identifier)\n",
    "    print(f\"Successfully read sheet: {passengers_sheet_identifier} ('Passengers').\")\n",
    "\n",
    "    df_passengers = df_passengers.rename(columns=passengers_rename_map, errors='ignore')\n",
    "    print(\"Renamed columns in Passengers DataFrame.\")\n",
    "\n",
    "    print(\"\\nFinished reading Excel file and renaming columns.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The Excel file was not found at {excel_file_path}. Please ensure the file name and path are correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the Excel file or renaming columns: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_csv_path = \"all_pdfs_extracted_data.csv\"\n",
    "\n",
    "print(f\"Starting filtering of Capacity and Passenger data based on train numbers from: {main_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_main_train_numbers = pd.read_csv(main_csv_path)\n",
    "\n",
    "    train_numbers_from_csv_series = df_main_train_numbers['Train Number']\n",
    "\n",
    "    train_numbers_from_csv = train_numbers_from_csv_series.dropna().astype(str).str.strip().unique()\n",
    "    print(f\"Found {len(train_numbers_from_csv)} unique train numbers in {main_csv_path}.\")\n",
    "\n",
    "    train_no_cap_series = df_capacity['TRAIN NO']\n",
    "    train_no_pass_series = df_passengers['TRAIN NO']\n",
    "\n",
    "    df_capacity['TRAIN NO_str'] = train_no_cap_series.dropna().astype(str).str.strip()\n",
    "    df_capacity_filtered = df_capacity[df_capacity['TRAIN NO_str'].isin(train_numbers_from_csv)].copy()\n",
    "    df_capacity_filtered = df_capacity_filtered.drop(columns=['TRAIN NO_str'])\n",
    "    print(f\"Filtered Capacity data. {len(df_capacity_filtered)} rows matching train numbers from CSV.\")\n",
    "\n",
    "    df_passengers['TRAIN NO_str'] = train_no_pass_series.dropna().astype(str).str.strip()\n",
    "    df_passengers_filtered = df_passengers[df_passengers['TRAIN NO_str'].isin(train_numbers_from_csv)].copy()\n",
    "    df_passengers_filtered = df_passengers_filtered.drop(columns=['TRAIN NO_str'])\n",
    "    print(f\"Filtered Passengers data. {len(df_passengers_filtered)} rows matching train numbers from CSV.\")\n",
    "\n",
    "    print(\"\\nFiltering complete.\")\n",
    "    print(f\"Filtered Capacity DataFrame shape: {df_capacity_filtered.shape}\")\n",
    "    print(f\"Filtered Passengers DataFrame shape: {df_passengers_filtered.shape}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"A required file was not found: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"A required column was not found: {e}. Please check column names in your Excel and CSV files.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during filtering: {e}\")\n",
    "    print(\"Please ensure the Excel file was read correctly in the previous step and has a 'TRAIN NO' column, and that the main CSV file exists and has a 'Train Number' column.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_cols_to_aggregate = [col for col in df_capacity_filtered.columns if col != 'TRAIN NO']\n",
    "passengers_cols_to_aggregate = [col for col in df_passengers_filtered.columns if col != 'TRAIN NO']\n",
    "\n",
    "df_capacity_aggregated = df_capacity_filtered.loc[:, capacity_cols_to_aggregate].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(0).copy()\n",
    "df_capacity_aggregated['TRAIN NO'] = df_capacity_filtered['TRAIN NO']\n",
    "df_capacity_aggregated = df_capacity_aggregated.groupby('TRAIN NO')[capacity_cols_to_aggregate].median().reset_index()\n",
    "\n",
    "df_passengers_aggregated = df_passengers_filtered.loc[:, passengers_cols_to_aggregate].apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(0).copy()\n",
    "df_passengers_aggregated['TRAIN NO'] = df_passengers_filtered['TRAIN NO']\n",
    "df_passengers_aggregated = df_passengers_aggregated.groupby('TRAIN NO')[passengers_cols_to_aggregate].median().reset_index()\n",
    "\n",
    "print(\"Aggregation complete.\")\n",
    "print(f\"Aggregated Capacity DataFrame shape: {df_capacity_aggregated.shape}\")\n",
    "print(f\"Aggregated Passengers DataFrame shape: {df_passengers_aggregated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65322f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_csv_path = \"all_pdfs_extracted_data.csv\"\n",
    "df_merged = pd.merge(\n",
    "    pd.merge(\n",
    "        pd.read_csv(main_csv_path), \n",
    "        df_capacity_aggregated[['TRAIN NO'] + [col for col in df_capacity_aggregated.columns if col.startswith('CAP')]], \n",
    "        how='left',\n",
    "        left_on='Train Number', \n",
    "        right_on='TRAIN NO' \n",
    "    ).drop(columns=['TRAIN NO'], errors='ignore'), \n",
    "    df_passengers_aggregated[['TRAIN NO'] + [col for col in df_passengers_aggregated.columns if col.startswith('OCC')]], \n",
    "    how='left',\n",
    "    left_on='Train Number', \n",
    "    right_on='TRAIN NO' \n",
    ").drop(columns=['TRAIN NO'], errors='ignore') \n",
    "\n",
    "print(\"Merged DataFrames.\")\n",
    "print(f\"Merged DataFrame shape: {df_merged.shape}\")\n",
    "df_merged.to_csv('all_pdfs_extracted_data_with_capacity_and_passengers.csv', index=False)\n",
    "print(\"Saved merged DataFrame to 'all_pdfs_extracted_data_with_capacity_and_passengers.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting data transformations on df_merged DataFrame...\")\n",
    "\n",
    "# --- 1. Train Type Encoding ---\n",
    "if 'Train Type' in df_merged.columns:\n",
    "    df_merged['Train Type Encoded'], train_type_uniques = pd.factorize(df_merged['Train Type'])\n",
    "    print(\"Encoded 'Train Type'. New column 'Train Type Encoded'.\")\n",
    "    print(\"Train Type Mapping:\", dict(zip(train_type_uniques, range(len(train_type_uniques)))))\n",
    "else:\n",
    "    print(\"Warning: 'Train Type' column not found in df_merged. Skipping encoding.\")\n",
    "    df_merged['Train Type Encoded'] = -1 \n",
    "\n",
    "\n",
    "# --- 2. Travel Days Calculation ---\n",
    "def count_travel_days(days_str):\n",
    "    if pd.isna(days_str) or str(days_str).strip() == '':\n",
    "        return 0 \n",
    "    \n",
    "    days_str_lower = str(days_str).lower() \n",
    "    if 'daily' in days_str_lower or 'all days' in days_str_lower:\n",
    "        return 7\n",
    "    \n",
    "    days_list = [day.strip() for day in days_str.split(',') if day.strip()]\n",
    "    return len(days_list)\n",
    "\n",
    "if 'Days of Operation' in df_merged.columns:\n",
    "    df_merged['Travel Days'] = df_merged['Days of Operation'].apply(count_travel_days)\n",
    "    print(\"Calculated 'Travel Days'. New column 'Travel Days'.\")\n",
    "else:\n",
    "    print(\"Warning: 'Days of Operation' column not found in df_merged. Skipping 'Travel Days' calculation.\")\n",
    "    df_merged['Travel Days'] = 0 \n",
    "\n",
    "# --- 4. Zone of Origin Encoding ---\n",
    "if 'Zone of Origin' in df_merged.columns:\n",
    "    df_merged['Zone of Origin Encoded'], zone_origin_uniques = pd.factorize(df_merged['Zone of Origin'])\n",
    "    print(\"Encoded 'Zone of Origin'. New column 'Zone of Origin Encoded'.\")\n",
    "    print(\"Zone of Origin Mapping:\", dict(zip(zone_origin_uniques, range(len(zone_origin_uniques)))))\n",
    "else:\n",
    "    print(\"Warning: 'Zone of Origin' column not found in df_merged. Skipping encoding.\")\n",
    "    df_merged['Zone of Origin Encoded'] = -1 \n",
    "\n",
    "\n",
    "# --- 5. Zone of Destination Encoding ---\n",
    "if 'Zone of Destination' in df_merged.columns:\n",
    "    df_merged['Zone of Destination Encoded'], zone_destination_uniques = pd.factorize(df_merged['Zone of Destination'])\n",
    "    print(\"Encoded 'Zone of Destination'. New column 'Zone of Destination Encoded'.\")\n",
    "    print(\"Zone of Destination Mapping:\", dict(zip(zone_destination_uniques, range(len(zone_destination_uniques)))))\n",
    "else:\n",
    "    print(\"Warning: 'Zone of Destination' column not found in df_merged. Skipping encoding.\")\n",
    "    df_merged['Zone of Destination Encoded'] = -1 \n",
    "\n",
    "print(\"\\nAll transformations complete on df_merged.\")\n",
    "print(df_merged.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_weekend_weekday(days_str):\n",
    "    if pd.isna(days_str) or str(days_str).strip() == '':\n",
    "        return None \n",
    "\n",
    "    days_str_lower = str(days_str).strip().lower() \n",
    "\n",
    "    if 'daily' in days_str_lower or 'all days' in days_str_lower:\n",
    "        return 20 \n",
    "\n",
    "    days_list_cleaned_lower = [day.strip().lower() for day in days_str.split(',') if day.strip()]\n",
    "\n",
    "    has_weekend = any(day in ['sun', 'sat'] for day in days_list_cleaned_lower)\n",
    "\n",
    "    if has_weekend:\n",
    "        return 20 \n",
    "\n",
    "    weekday_abbreviations = ['mon', 'tue', 'wed', 'thu', 'fri']\n",
    "    has_weekday = any(day in weekday_abbreviations for day in days_list_cleaned_lower)\n",
    "\n",
    "    if has_weekday:\n",
    "         return 10 \n",
    "\n",
    "    return None \n",
    "\n",
    "if 'Days of Operation' in df_merged.columns:\n",
    "    df_merged['Weekend/Day'] = df_merged['Days of Operation'].astype(str).apply(categorize_weekend_weekday)\n",
    "else:\n",
    "    df_merged['Weekend/Day'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_time(t):\n",
    "    if pd.isnull(t):\n",
    "        return None  \n",
    "    hour = int(t.split(\":\")[0])\n",
    "    if 4 <= hour <= 18:\n",
    "        return 10  # Day\n",
    "    else:\n",
    "        return 20  # Night\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_merged['Night/Day-Dep'] = df_merged['Dep Time'].apply(classify_time)\n",
    "df_merged['Night/Day-Arr'] = df_merged['Arr Time'].apply(classify_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833995d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('trainfulldata.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
